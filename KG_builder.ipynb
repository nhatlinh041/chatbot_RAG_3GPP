{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3GPP Knowledge Graph Builder v2 - Simple Version\n",
    "\n",
    "This is a simplified version that completely avoids null property issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install neo4j tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Optional\n",
    "from neo4j import GraphDatabase\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "\n",
    "class SimpleKGProcessorV2:\n",
    "    def __init__(self, neo4j_uri: str, neo4j_user: str, neo4j_password: str):\n",
    "        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "        self.documents = {}\n",
    "        self.chunks = []\n",
    "        \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "    \n",
    "    def clear_database(self):\n",
    "        \"\"\"Clear all existing data\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            print(\"Database cleared successfully!\")\n",
    "    \n",
    "    def test_connection(self):\n",
    "        \"\"\"Test Neo4j connection\"\"\"\n",
    "        try:\n",
    "            with self.driver.session() as session:\n",
    "                result = session.run(\"RETURN 1 as test\")\n",
    "                print(\"✅ Neo4j connection successful!\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Connection failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_json_files(self, json_dir: str):\n",
    "        \"\"\"Load all JSON files from directory\"\"\"\n",
    "        json_path = Path(json_dir)\n",
    "        self.documents.clear()\n",
    "        self.chunks.clear()\n",
    "        \n",
    "        for json_file in json_path.glob(\"*.json\"):\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                spec_id = data[\"metadata\"][\"specification_id\"]\n",
    "                self.documents[spec_id] = data\n",
    "                self.chunks.extend(data[\"chunks\"])\n",
    "        print(f\"Loaded {len(self.documents)} documents with {len(self.chunks)} chunks\")\n",
    "    \n",
    "    def build_knowledge_graph(self):\n",
    "        \"\"\"Build knowledge graph - simple version\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Create constraints\n",
    "            try:\n",
    "                session.run(\"CREATE CONSTRAINT doc_id IF NOT EXISTS FOR (d:Document) REQUIRE d.spec_id IS UNIQUE\")\n",
    "                session.run(\"CREATE CONSTRAINT chunk_id IF NOT EXISTS FOR (c:Chunk) REQUIRE c.chunk_id IS UNIQUE\")\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Create documents\n",
    "            self.create_documents(session)\n",
    "            \n",
    "            # Create chunks\n",
    "            self.create_chunks(session)\n",
    "            \n",
    "            # Create simple references - NO null properties ever\n",
    "            self.create_simple_references(session)\n",
    "    \n",
    "    def create_documents(self, session):\n",
    "        \"\"\"Create document nodes\"\"\"\n",
    "        print(\"Creating documents...\")\n",
    "        for spec_id, data in tqdm(self.documents.items()):\n",
    "            session.run(\"\"\"\n",
    "                MERGE (d:Document {spec_id: $spec_id})\n",
    "                SET d.version = $version,\n",
    "                    d.title = $title,\n",
    "                    d.total_chunks = $total_chunks\n",
    "            \"\"\", \n",
    "                spec_id=spec_id,\n",
    "                version=data[\"metadata\"][\"version\"],\n",
    "                title=data[\"metadata\"][\"title\"],\n",
    "                total_chunks=data[\"export_info\"][\"total_chunks\"]\n",
    "            )\n",
    "    \n",
    "    def create_chunks(self, session):\n",
    "        \"\"\"Create chunk nodes\"\"\"\n",
    "        print(\"Creating chunks...\")\n",
    "        for chunk in tqdm(self.chunks):\n",
    "            # Extract spec_id\n",
    "            chunk_id_parts = chunk[\"chunk_id\"].split(\"_\")\n",
    "            if len(chunk_id_parts) >= 3:\n",
    "                spec_id = f\"{chunk_id_parts[0]}_{chunk_id_parts[1]}.{chunk_id_parts[2]}\"\n",
    "            else:\n",
    "                spec_id = chunk[\"chunk_id\"]\n",
    "            \n",
    "            # Get metadata safely\n",
    "            content_meta = chunk.get(\"content_metadata\", {})\n",
    "            \n",
    "            session.run(\"\"\"\n",
    "                MERGE (c:Chunk {chunk_id: $chunk_id})\n",
    "                SET c.section_id = $section_id,\n",
    "                    c.section_title = $section_title,\n",
    "                    c.content = $content,\n",
    "                    c.chunk_type = $chunk_type,\n",
    "                    c.spec_id = $spec_id,\n",
    "                    c.word_count = $word_count,\n",
    "                    c.complexity_score = $complexity_score,\n",
    "                    c.key_terms = $key_terms\n",
    "            \"\"\", \n",
    "                chunk_id=chunk[\"chunk_id\"],\n",
    "                section_id=chunk[\"section_id\"],\n",
    "                section_title=chunk[\"section_title\"],\n",
    "                content=chunk[\"content\"],\n",
    "                chunk_type=chunk[\"chunk_type\"],\n",
    "                spec_id=spec_id,\n",
    "                word_count=content_meta.get(\"word_count\", 0),\n",
    "                complexity_score=content_meta.get(\"complexity_score\", 0.0),\n",
    "                key_terms=content_meta.get(\"key_terms\", [])\n",
    "            )\n",
    "        \n",
    "        # Create CONTAINS relationships\n",
    "        print(\"Creating CONTAINS relationships...\")\n",
    "        session.run(\"\"\"\n",
    "            MATCH (d:Document), (c:Chunk)\n",
    "            WHERE d.spec_id = c.spec_id\n",
    "            MERGE (d)-[:CONTAINS]->(c)\n",
    "        \"\"\")\n",
    "    \n",
    "    def create_simple_references(self, session):\n",
    "        \"\"\"Create references - completely avoiding null properties\"\"\"\n",
    "        print(\"Creating references...\")\n",
    "        \n",
    "        for chunk in tqdm(self.chunks):\n",
    "            source_chunk_id = chunk[\"chunk_id\"]\n",
    "            cross_refs = chunk.get(\"cross_references\", {})\n",
    "            \n",
    "            # Process external references\n",
    "            for ref in cross_refs.get(\"external\", []):\n",
    "                self.create_safe_external_reference(session, source_chunk_id, ref)\n",
    "    \n",
    "    def create_safe_external_reference(self, session, source_chunk_id: str, ref: dict):\n",
    "        \"\"\"Create external reference with zero null properties\"\"\"\n",
    "        \n",
    "        # Generate unique ID\n",
    "        ref_uid = hashlib.md5(f\"{source_chunk_id}_{ref['target_spec']}_{ref['ref_id']}\".encode()).hexdigest()[:10]\n",
    "        \n",
    "        # Get target_item safely\n",
    "        target_item = ref.get(\"target_item\")\n",
    "        \n",
    "        # Only create if target document exists\n",
    "        result = session.run(\"\"\"\n",
    "            MATCH (source:Chunk {chunk_id: $source_id})\n",
    "            MATCH (target_doc:Document {spec_id: $target_spec})\n",
    "            RETURN count(*) as exists\n",
    "        \"\"\", source_id=source_chunk_id, target_spec=ref[\"target_spec\"])\n",
    "        \n",
    "        if result.single()[\"exists\"] > 0:\n",
    "            # Create basic relationship with NO null properties\n",
    "            session.run(\"\"\"\n",
    "                MATCH (source:Chunk {chunk_id: $source_id})\n",
    "                MATCH (target_doc:Document {spec_id: $target_spec})\n",
    "                CREATE (source)-[r:REFERENCES_SPEC]->(target_doc)\n",
    "                SET r.ref_id = $ref_id,\n",
    "                    r.ref_type = $ref_type,\n",
    "                    r.confidence = $confidence,\n",
    "                    r.is_external = true,\n",
    "                    r.ref_uid = $ref_uid\n",
    "            \"\"\", \n",
    "                source_id=source_chunk_id,\n",
    "                target_spec=ref[\"target_spec\"],\n",
    "                ref_id=ref[\"ref_id\"],\n",
    "                ref_type=ref[\"ref_type\"],\n",
    "                confidence=ref[\"confidence\"],\n",
    "                ref_uid=ref_uid\n",
    "            )\n",
    "            \n",
    "            # Add target_item ONLY if it exists and is valid\n",
    "            if target_item and target_item != \"null\" and target_item.strip():\n",
    "                session.run(\"\"\"\n",
    "                    MATCH ()-[r:REFERENCES_SPEC]->()\n",
    "                    WHERE r.ref_uid = $ref_uid\n",
    "                    SET r.target_item = $target_item\n",
    "                \"\"\", \n",
    "                    ref_uid=ref_uid,\n",
    "                    target_item=target_item\n",
    "                )\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get database statistics\"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            stats = {}\n",
    "            \n",
    "            # Count nodes\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (n) \n",
    "                RETURN labels(n)[0] as label, count(n) as count\n",
    "            \"\"\")\n",
    "            \n",
    "            for record in result:\n",
    "                stats[record['label']] = record['count']\n",
    "            \n",
    "            # Count relationships\n",
    "            rel_result = session.run(\"\"\"\n",
    "                MATCH ()-[r]->() \n",
    "                RETURN type(r) as rel_type, count(r) as count\n",
    "            \"\"\")\n",
    "            \n",
    "            stats['relationships'] = {}\n",
    "            for record in rel_result:\n",
    "                stats['relationships'][record['rel_type']] = record['count']\n",
    "            \n",
    "            return stats\n",
    "    \n",
    "    def process_json_to_kg_simple(self, json_dir: str, clear_first: bool = True):\n",
    "        \"\"\"Simple pipeline from JSON v2 to Knowledge Graph\"\"\"\n",
    "        if not self.test_connection():\n",
    "            return False\n",
    "        \n",
    "        if clear_first:\n",
    "            print(\"Clearing existing database...\")\n",
    "            self.clear_database()\n",
    "            \n",
    "        print(\"Loading JSON v2 files...\")\n",
    "        self.load_json_files(json_dir)\n",
    "        \n",
    "        print(\"Building simple knowledge graph...\")\n",
    "        self.build_knowledge_graph()\n",
    "        \n",
    "        print(\"Simple knowledge graph built successfully!\")\n",
    "        \n",
    "        # Show statistics\n",
    "        stats = self.get_statistics()\n",
    "        print(f\"\\nDatabase Statistics:\")\n",
    "        for node_type, count in stats.items():\n",
    "            if node_type != 'relationships':\n",
    "                print(f\"  {node_type}: {count} nodes\")\n",
    "        \n",
    "        print(\"  Relationships:\")\n",
    "        for rel_type, count in stats.get('relationships', {}).items():\n",
    "            print(f\"    {rel_type}: {count}\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "print(\"Simple KG Processor V2 loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Simple Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simple processor\n",
    "processor = SimpleKGProcessorV2(\n",
    "    neo4j_uri=\"neo4j://localhost:7687\",\n",
    "    neo4j_user=\"neo4j\",\n",
    "    neo4j_password=\"password\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Process the v2 JSON files\n",
    "    success = processor.process_json_to_kg_simple(\n",
    "        \"/home/linguyen/3GPP/3GPP_JSON_DOC/processed_json_v2/\", \n",
    "        clear_first=True\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Simple Knowledge Graph built successfully!\")\n",
    "        print(\"This version completely avoids null property issues.\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "finally:\n",
    "    processor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Simple Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test basic queries\nprocessor = SimpleKGProcessorV2(\n    neo4j_uri=\"neo4j://localhost:7687\",\n    neo4j_user=\"neo4j\",\n    neo4j_password=\"password\"\n)\n\ntry:\n    with processor.driver.session() as session:\n        print(\"Testing simple queries:\")\n        print(\"=\"*30)\n        \n        # Find chunks about SCP\n        result = session.run(\"\"\"\n            MATCH (c:Chunk)\n            WHERE toLower(c.content) CONTAINS 'scp'\n            RETURN c.spec_id as spec, c.section_title as title, c.chunk_type as type\n            LIMIT 5\n        \"\"\")\n        \n        print(\"\\nChunks containing 'SCP':\")\n        for record in result:\n            print(f\"  {record['spec']} - {record['title']} ({record['type']})\")\n        \n        # Find references\n        result = session.run(\"\"\"\n            MATCH (source:Chunk)-[r:REFERENCES_SPEC]->(target:Document)\n            RETURN source.spec_id as from_spec, target.spec_id as to_spec, r.confidence as confidence\n            ORDER BY confidence DESC\n            LIMIT 5\n        \"\"\")\n        \n        print(\"\\nTop references by confidence:\")\n        for record in result:\n            print(f\"  {record['from_spec']} → {record['to_spec']} (confidence: {record['confidence']:.2f})\")\n\nfinally:\n    processor.close()\n\nprint(\"\\n✅ Simple Knowledge Graph is working perfectly!\")\nprint(\"No null property errors - completely safe approach.\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Fix Missing REFERENCES_CHUNK Relationships\n\nThis section adds the missing REFERENCES_CHUNK relationships based on internal cross-references from the JSON data.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class ReferencesFixer:\n    \"\"\"Fix missing REFERENCES_CHUNK relationships in the knowledge graph\"\"\"\n    \n    def __init__(self, neo4j_uri: str = \"neo4j://localhost:7687\", \n                 neo4j_user: str = \"neo4j\", neo4j_password: str = \"password\"):\n        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n        self.chunks_data = {}\n    \n    def close(self):\n        self.driver.close()\n    \n    def load_processed_json(self, json_dir: str):\n        \"\"\"Load processed JSON files to get internal references\"\"\"\n        json_path = Path(json_dir)\n        \n        for json_file in json_path.glob(\"*.json\"):\n            try:\n                with open(json_file, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                    \n                for chunk in data.get(\"chunks\", []):\n                    chunk_id = chunk[\"chunk_id\"]\n                    self.chunks_data[chunk_id] = chunk\n                    \n            except Exception as e:\n                print(f\"Error loading {json_file}: {e}\")\n        \n        print(f\"Loaded {len(self.chunks_data)} chunks from JSON files\")\n    \n    def add_missing_chunk_references(self):\n        \"\"\"Add REFERENCES_CHUNK relationships based on internal cross-references\"\"\"\n        print(\"Adding missing REFERENCES_CHUNK relationships...\")\n        \n        with self.driver.session() as session:\n            added_count = 0\n            \n            for chunk_id, chunk_data in tqdm(self.chunks_data.items()):\n                cross_refs = chunk_data.get(\"cross_references\", {})\n                internal_refs = cross_refs.get(\"internal\", [])\n                \n                for ref in internal_refs:\n                    target_chunk_id = ref.get(\"target_chunk_id\")\n                    if target_chunk_id:\n                        success = self._create_chunk_reference(\n                            session, chunk_id, target_chunk_id, ref\n                        )\n                        if success:\n                            added_count += 1\n            \n            print(f\"Added {added_count} REFERENCES_CHUNK relationships\")\n    \n    def _create_chunk_reference(self, session, source_chunk_id: str, \n                               target_chunk_id: str, ref_data: dict) -> bool:\n        \"\"\"Create a REFERENCES_CHUNK relationship between two chunks\"\"\"\n        try:\n            # Generate unique reference ID\n            ref_uid = hashlib.md5(\n                f\"{source_chunk_id}_{target_chunk_id}_{ref_data.get('ref_id', '')}\".encode()\n            ).hexdigest()[:10]\n            \n            # Check if both chunks exist\n            result = session.run(\"\"\"\n                MATCH (source:Chunk {chunk_id: $source_id})\n                MATCH (target:Chunk {chunk_id: $target_id})\n                RETURN count(*) as exists\n            \"\"\", source_id=source_chunk_id, target_id=target_chunk_id)\n            \n            if result.single()[\"exists\"] == 0:\n                return False\n            \n            # Create the relationship if it doesn't exist\n            session.run(\"\"\"\n                MATCH (source:Chunk {chunk_id: $source_id})\n                MATCH (target:Chunk {chunk_id: $target_id})\n                MERGE (source)-[r:REFERENCES_CHUNK {ref_uid: $ref_uid}]->(target)\n                SET r.ref_id = $ref_id,\n                    r.ref_type = $ref_type,\n                    r.confidence = $confidence,\n                    r.is_internal = true\n            \"\"\", \n                source_id=source_chunk_id,\n                target_id=target_chunk_id,\n                ref_uid=ref_uid,\n                ref_id=ref_data.get(\"ref_id\", \"\"),\n                ref_type=ref_data.get(\"ref_type\", \"internal\"),\n                confidence=ref_data.get(\"confidence\", 0.8)\n            )\n            \n            return True\n            \n        except Exception as e:\n            print(f\"Error creating reference {source_chunk_id} -> {target_chunk_id}: {e}\")\n            return False\n    \n    def verify_references(self):\n        \"\"\"Verify that REFERENCES_CHUNK relationships exist\"\"\"\n        with self.driver.session() as session:\n            # Count REFERENCES_CHUNK relationships\n            result = session.run(\"\"\"\n                MATCH ()-[r:REFERENCES_CHUNK]->()\n                RETURN count(r) as chunk_ref_count\n            \"\"\")\n            chunk_ref_count = result.single()[\"chunk_ref_count\"]\n            \n            # Count REFERENCES_SPEC relationships\n            result = session.run(\"\"\"\n                MATCH ()-[r:REFERENCES_SPEC]->()\n                RETURN count(r) as spec_ref_count\n            \"\"\")\n            spec_ref_count = result.single()[\"spec_ref_count\"]\n            \n            print(f\"Database relationship counts:\")\n            print(f\"  REFERENCES_CHUNK: {chunk_ref_count}\")\n            print(f\"  REFERENCES_SPEC: {spec_ref_count}\")\n            \n            return chunk_ref_count > 0\n    \n    def fix_missing_references(self, json_dir: str = \"/home/linguyen/3GPP/3GPP_JSON_DOC/processed_json_v2/\"):\n        \"\"\"Main method to fix missing references\"\"\"\n        print(\"Loading processed JSON files...\")\n        self.load_processed_json(json_dir)\n        \n        print(\"Adding missing REFERENCES_CHUNK relationships...\")\n        self.add_missing_chunk_references()\n        \n        print(\"Verifying references...\")\n        success = self.verify_references()\n        \n        if success:\n            print(\"✅ Missing REFERENCES_CHUNK relationships added successfully!\")\n        else:\n            print(\"⚠️ No REFERENCES_CHUNK relationships found - check internal refs in JSON data\")\n        \n        return success\n\nprint(\"ReferencesFixer class loaded successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run the references fixer to add missing REFERENCES_CHUNK relationships\nfixer = ReferencesFixer(\n    neo4j_uri=\"neo4j://localhost:7687\",\n    neo4j_user=\"neo4j\",\n    neo4j_password=\"password\"\n)\n\ntry:\n    success = fixer.fix_missing_references(\n        \"/home/linguyen/3GPP/3GPP_JSON_DOC/processed_json_v2/\"\n    )\n    \n    if success:\n        print(\"\\n\" + \"=\"*50)\n        print(\"✅ Database references fixed successfully!\")\n        print(\"The REFERENCES_CHUNK relationships are now available.\")\n        print(\"=\"*50)\nfinally:\n    fixer.close()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Create Term Nodes from Abbreviations\n\nThis section extracts abbreviations and definitions from document chunks and creates Term nodes in the knowledge graph. Term nodes help resolve abbreviations like \"SCP\" to their full names like \"Service Communication Proxy\" and track which specs define them.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import sys\nsys.path.insert(0, '/home/linguyen/3GPP')\nfrom term_extractor import TermExtractor, ExtractedTerm\n\nclass TermNodeBuilder:\n    \"\"\"Build Term nodes in Neo4j from abbreviation/definition chunks\"\"\"\n    \n    def __init__(self, neo4j_uri: str = \"neo4j://localhost:7687\", \n                 neo4j_user: str = \"neo4j\", neo4j_password: str = \"password\"):\n        self.driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n        self.extractor = TermExtractor()\n        self.term_dict = {}  # Consolidated term dictionary\n    \n    def close(self):\n        self.driver.close()\n    \n    def extract_terms_from_chunks(self, json_dir: str):\n        \"\"\"Extract all terms from abbreviation/definition chunks in JSON files\"\"\"\n        print(\"Extracting terms from JSON files...\")\n        json_path = Path(json_dir)\n        \n        for json_file in tqdm(list(json_path.glob(\"*.json\"))):\n            try:\n                with open(json_file, 'r', encoding='utf-8') as f:\n                    data = json.load(f)\n                \n                spec_id = data[\"metadata\"][\"specification_id\"]\n                \n                for chunk in data.get(\"chunks\", []):\n                    section_title = chunk.get(\"section_title\", \"\").lower()\n                    content = chunk.get(\"content\", \"\")\n                    \n                    # Only process abbreviation/definition sections\n                    if 'abbreviation' in section_title:\n                        terms = self.extractor.extract_abbreviations(content, spec_id)\n                        self._merge_terms(terms)\n                    elif 'definition' in section_title:\n                        terms = self.extractor.extract_definitions(content, spec_id)\n                        self._merge_terms(terms)\n                        \n            except Exception as e:\n                print(f\"Error processing {json_file}: {e}\")\n        \n        print(f\"Extracted {len(self.term_dict)} unique terms\")\n        return self.term_dict\n    \n    def _merge_terms(self, terms: list):\n        \"\"\"Merge terms into consolidated dictionary\"\"\"\n        for term in terms:\n            abbr = term.abbreviation\n            \n            if abbr not in self.term_dict:\n                self.term_dict[abbr] = {\n                    'abbreviation': abbr,\n                    'full_name': term.full_name,\n                    'term_type': term.term_type,\n                    'source_specs': [term.source_spec],\n                    'primary_spec': term.source_spec\n                }\n            else:\n                # Add source spec if not already present\n                if term.source_spec not in self.term_dict[abbr]['source_specs']:\n                    self.term_dict[abbr]['source_specs'].append(term.source_spec)\n    \n    def create_term_constraint(self, session):\n        \"\"\"Create unique constraint for Term nodes\"\"\"\n        try:\n            session.run(\"CREATE CONSTRAINT term_abbr IF NOT EXISTS FOR (t:Term) REQUIRE t.abbreviation IS UNIQUE\")\n            print(\"Term constraint created\")\n        except Exception as e:\n            print(f\"Constraint may already exist: {e}\")\n    \n    def create_term_nodes(self):\n        \"\"\"Create Term nodes and DEFINED_IN relationships in Neo4j\"\"\"\n        print(\"Creating Term nodes in Neo4j...\")\n        \n        with self.driver.session() as session:\n            # Create constraint\n            self.create_term_constraint(session)\n            \n            created_count = 0\n            for abbr, term_data in tqdm(self.term_dict.items()):\n                try:\n                    # Create Term node\n                    session.run(\"\"\"\n                        MERGE (t:Term {abbreviation: $abbr})\n                        SET t.full_name = $full_name,\n                            t.term_type = $term_type,\n                            t.source_specs = $source_specs,\n                            t.primary_spec = $primary_spec\n                    \"\"\",\n                        abbr=abbr,\n                        full_name=term_data['full_name'],\n                        term_type=term_data['term_type'],\n                        source_specs=term_data['source_specs'],\n                        primary_spec=term_data['primary_spec']\n                    )\n                    \n                    # Create DEFINED_IN relationships to Documents\n                    for spec_id in term_data['source_specs']:\n                        session.run(\"\"\"\n                            MATCH (t:Term {abbreviation: $abbr})\n                            MATCH (d:Document {spec_id: $spec_id})\n                            MERGE (t)-[:DEFINED_IN]->(d)\n                        \"\"\",\n                            abbr=abbr,\n                            spec_id=spec_id\n                        )\n                    \n                    created_count += 1\n                    \n                except Exception as e:\n                    print(f\"Error creating term '{abbr}': {e}\")\n            \n            print(f\"Created {created_count} Term nodes\")\n    \n    def verify_terms(self):\n        \"\"\"Verify Term nodes were created\"\"\"\n        with self.driver.session() as session:\n            # Count Term nodes\n            result = session.run(\"MATCH (t:Term) RETURN count(t) as count\")\n            term_count = result.single()[\"count\"]\n            \n            # Count DEFINED_IN relationships\n            result = session.run(\"MATCH ()-[r:DEFINED_IN]->() RETURN count(r) as count\")\n            rel_count = result.single()[\"count\"]\n            \n            # Sample some terms\n            result = session.run(\"\"\"\n                MATCH (t:Term)\n                WHERE t.abbreviation IN ['SCP', 'AMF', 'SMF', 'UPF', 'PCF']\n                RETURN t.abbreviation as abbr, t.full_name as full_name, \n                       t.source_specs as specs\n                ORDER BY t.abbreviation\n            \"\"\")\n            \n            print(f\"\\nTerm Node Statistics:\")\n            print(f\"  Total Term nodes: {term_count}\")\n            print(f\"  DEFINED_IN relationships: {rel_count}\")\n            print(f\"\\nSample 5G Core NF Terms:\")\n            for record in result:\n                specs = record['specs'][:3] if len(record['specs']) > 3 else record['specs']\n                print(f\"  {record['abbr']}: {record['full_name']}\")\n                print(f\"    Defined in: {', '.join(specs)}{'...' if len(record['specs']) > 3 else ''}\")\n            \n            return term_count > 0\n    \n    def build_term_nodes(self, json_dir: str = \"/home/linguyen/3GPP/3GPP_JSON_DOC/processed_json_v2/\"):\n        \"\"\"Main method to build Term nodes from JSON files\"\"\"\n        print(\"=\"*50)\n        print(\"Building Term Nodes from Abbreviations/Definitions\")\n        print(\"=\"*50)\n        \n        # Extract terms\n        self.extract_terms_from_chunks(json_dir)\n        \n        # Create nodes in Neo4j\n        self.create_term_nodes()\n        \n        # Verify\n        success = self.verify_terms()\n        \n        if success:\n            print(\"\\n✅ Term nodes created successfully!\")\n        else:\n            print(\"\\n⚠️ No Term nodes created - check JSON files\")\n        \n        return success\n\nprint(\"TermNodeBuilder class loaded successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run the Term Node Builder to create Term nodes from abbreviations\nterm_builder = TermNodeBuilder(\n    neo4j_uri=\"neo4j://localhost:7687\",\n    neo4j_user=\"neo4j\",\n    neo4j_password=\"password\"\n)\n\ntry:\n    success = term_builder.build_term_nodes(\n        \"/home/linguyen/3GPP/3GPP_JSON_DOC/processed_json_v2/\"\n    )\n    \n    if success:\n        print(\"\\n\" + \"=\"*50)\n        print(\"✅ Term nodes added to Knowledge Graph!\")\n        print(\"You can now query terms like:\")\n        print(\"  MATCH (t:Term {abbreviation: 'SCP'}) RETURN t\")\n        print(\"=\"*50)\nfinally:\n    term_builder.close()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}